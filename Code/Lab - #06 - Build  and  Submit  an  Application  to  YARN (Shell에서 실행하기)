from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext

conf = SparkConf().setAppName("Word-Count") # only for pyspark-shell
sc = SparkContext(conf=conf) # only for pyspark-shell
baseRdd = sc.textFile('wasbs://ghtestbigdata@ghtestdatastacc.blob.core.windows.net/sparkhol/wordcount-data/Harry-Potter-and-the-Sorcerer.txt')

splitRdd = baseRdd.flatMap(lambda line: line.split(" "))
mappedRdd = splitRdd.map(lambda line: (line,1))

reducedRdd = mappedRdd.reduceByKey(lambda a,b: a+b)
reducedRdd.sortBy(lambda x: x[1], False).take(10)
